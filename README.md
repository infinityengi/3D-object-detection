# ğŸš—ğŸ” 3D Object Detection with PointPillars

This repository is a **reproducible, extensible, and educational framework** for 3D object detection and semantic segmentation projects. It builds on assignments from **RWTH Aachen Universityâ€™s Institute for Automotive Engineering (ika)** as part of the
[**Automated and Connected Driving Challenges (ACDC)** MOOC](https://www.edx.org/course/automated-and-connected-driving-challenges).

The goal is to provide a **self-contained guide and toolkit** for image segmentation and object detection projects in autonomous driving â€” from data preprocessing and model training to experiments and literature review.

---

## ğŸ“‚ Repository Structure

```
3D-object-detection/
â”‚
â”œâ”€â”€ docker/                     # Docker-related files
â”‚   â”œâ”€â”€ Dockerfile              # Build reproducible environments
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ run.sh                  # Convenience script for launching container
â”‚
â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â”‚   â”œâ”€â”€ 3D Object Detection.ipynb
â”‚   â”œâ”€â”€ assets/                 # Figures, plots, images for notebooks
â”‚   â”œâ”€â”€ datasets/               # Datasets (e.g., KITTI samples)
â”‚   â”œâ”€â”€ grid_mapping/           
â”‚   â”œâ”€â”€ ipm_assets/             
â”‚   â”œâ”€â”€ localization/           
â”‚   â”œâ”€â”€ object_detection/       # Custom object detection code
â”‚   â”œâ”€â”€ segmentation_utils/     
â”‚   â””â”€â”€ tensorflow_datasets/    
â”‚
â”œâ”€â”€ experiments/                # Saved experiments, configs, logs
â”‚   â”œâ”€â”€ runs/                   
â”‚   â””â”€â”€ configs/                
â”‚
â”œâ”€â”€ literature/                 # Research papers, notes, references
â”‚   â”œâ”€â”€ papers/                 
â”‚   â””â”€â”€ summaries.md            
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

> âœ… This structure is designed to evolve into a **general framework for image segmentation** using ML, with clear separation of **datasets, models, experiments, and literature**.

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/3D-object-detection.git
cd 3D-object-detection
```

### 2. Build the Docker Container

```bash
cd docker
docker build -t 3d-object-detection .
```

### 3. Run the Container

```bash
bash docker/run.sh
```

This script will mount the repository into the container and launch JupyterLab.

### 4. Open JupyterLab

Inside the container:

```bash
jupyter lab --ip=0.0.0.0 --port=8888 --allow-root
```

Open the link shown in your terminal to access the notebooks in your browser.

---

## ğŸ“’ Notebooks Overview

### **01_data_preprocessing.ipynb**

* Prepares KITTI dataset samples.
* Explores **LiDAR point clouds and annotations**.
* Visualizes 3D bounding boxes in **2D birdâ€™s-eye view**.
* Provides functions for **coordinate transformations** and **label projections**.

---

### **02_model_training.ipynb**

* Implements **PointPillars** for 3D object detection on KITTI.
* Inspects hyperparameters (e.g., anchors, batch sizes, loss weights).
* Prepares training and inference workflows.
* Visualizes detection results and bounding box predictions.

---

## ğŸ›£ï¸ Roadmap: From Object Detection to Semantic Segmentation

This repository is a **stepping stone toward full semantic segmentation** workflows. Suggested roadmap:

1. **Data Preparation**

   * Collect and preprocess raw datasets (KITTI, Waymo, nuScenes, Cityscapes).
   * Apply augmentations and dataset splits.

2. **Object Detection (Current Stage)**

   * Explore **PointPillars** for LiDAR-based 3D object detection.
   * Train and validate detection pipelines.

3. **Semantic Segmentation**

   * Introduce 2D/3D segmentation models (U-Net, DeepLab, RangeNet++).
   * Label and evaluate per-pixel classifications.

4. **Experiment Management**

   * Track experiments in `experiments/`.
   * Save configs, results, and logs systematically.

5. **Future Extensions**

   * Integrate multiple datasets.
   * Benchmark different models.
   * Add tools for visualization and deployment.

---

## ğŸ“š References & Acknowledgements

This repository builds upon assignments from:

* **Automated and Connected Driving Challenges (ACDC)** MOOC on [edX](https://www.edx.org/course/automated-and-connected-driving-challenges)
  by [RWTH Aachen University](https://rwth-aachen.de), taught by the
  [Institute for Automotive Engineering (ika)](https://www.ika.rwth-aachen.de/).

> ğŸ‘‰ **Enroll for free** [here](https://www.edx.org/course/automated-and-connected-driving-challenges) to learn more about automated and connected mobility!

Additional references:

* [PointPillars: Fast Encoders for Object Detection from Point Clouds](https://arxiv.org/abs/1812.05784)
* [KITTI 3D Object Detection Benchmark](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)

---

## ğŸ–¼ï¸ Visual Examples

<div align="center">
  <img src="notebooks/assets/2025-09-30_00-57.png" width="400">
  <img src="notebooks/assets/2025-09-30_01-12.png" width="400">
  <p><em>Visualization of LiDAR point clouds and 3D bounding boxes from KITTI.</em></p>
</div>

---

## ğŸ“œ License

This project is released under the **MIT License**. See [LICENSE](LICENSE) for details.
